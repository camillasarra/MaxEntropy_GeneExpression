{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca611956-73f0-41d1-ba6b-dc5433da1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import h5py\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from statistics import mode\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "\n",
    "from functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "\n",
    "folder_bin = folder = DATA_PATH\n",
    "folder_saved = f'{folder}/art_saved/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b13a39-b286-4e87-b60b-c41c8ff786a0",
   "metadata": {},
   "source": [
    "# Load and prepare data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e509603-5bdd-42d9-b2dc-5d977c94549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 8 #select the NUM_CLASSES largest classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd377689-916d-4a0b-be55-c367662551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(f'{folder}cell_metadata.csv')\n",
    "f = h5py.File(f'{folder}C57BL6J-638850-raw.h5ad', 'r')\n",
    "cell_id = f['obs']['cell_label'][:]\n",
    "cell_id = [cell_id[i].decode() for i in range(len(cell_id))]\n",
    "brain_section = f['obs']['brain_section_label']['codes'][:]\n",
    "version = '20231215'\n",
    "f = open(f'{folder}manifest.json')\n",
    "manifest = json.load(f)\n",
    "print(\"version: \", manifest['version'])\n",
    "file = f'{folder}cluster_to_cluster_annotation_membership_pivoted.csv'\n",
    "cluster_details = pd.read_csv(file,keep_default_na=False)\n",
    "cluster_details.set_index('cluster_alias', inplace=True)\n",
    "file = f'{folder}/cluster_to_cluster_annotation_membership_color.csv'\n",
    "cluster_colors = pd.read_csv(file,keep_default_na=False)\n",
    "cluster_colors.set_index('cluster_alias', inplace=True)\n",
    "metadata_extended = metadata.join(cluster_details,on='cluster_alias')\n",
    "metadata_extended = metadata_extended.join(cluster_colors,on='cluster_alias')\n",
    "heights = np.sort(np.array(list(dict.fromkeys(metadata_extended.z))))\n",
    "file = f'{folder}gene.csv.1'\n",
    "gene = pd.read_csv(file)\n",
    "gene.set_index('gene_identifier',inplace=True)\n",
    "print(\"Number of genes = \", len(gene))\n",
    "adata = ad.read_h5ad(f'{folder}C57BL6J-638850-raw.h5ad', backed='r')\n",
    "adatalog = ad.read_h5ad(f'{folder}C57BL6J-638850-log2.h5ad', backed='r')\n",
    "print('------\\nDONE: data successfully loaded\\n-----')\n",
    "section_names  = list(dict.fromkeys(metadata.brain_section_label.values))\n",
    "pred = [x in section_names for x in metadata_extended['brain_section_label']]\n",
    "sections = metadata_extended[pred] # this is (num_cells)x21\n",
    "print(np.array(pred).sum())\n",
    "gnames = gene['gene_symbol'].values[:100]\n",
    "pred = [x in gnames for x in adata.var.gene_symbol]\n",
    "gene_filtered = adata.var[pred]\n",
    "asubset = adata[:,gene_filtered.index].to_memory() #this is 4mln x (num_genes)\n",
    "asubsetlog = adatalog[:,gene_filtered.index].to_memory() #this is 4mln x (num_genes)\n",
    "gdata = asubset.to_df()\n",
    "gdatalog = asubsetlog.to_df()\n",
    "gdata.columns = gnames\n",
    "gdatalog.columns = gnames\n",
    "joined = sections.join(gdata, 'cell_label')\n",
    "joinedlog = sections.join(gdatalog, 'cell_label')\n",
    "data = joined[gnames].to_numpy()\n",
    "datalog = joinedlog[gnames].to_numpy()\n",
    "print('----------\\n DATA SUCCESSFULLY SELECTED\\n----------')\n",
    "# volumes = np.nanmean(data/(2**datalog-1),1) ### only for fraction\n",
    "del data\n",
    "del datalog\n",
    "section_names  = list(dict.fromkeys(metadata.brain_section_label.values))\n",
    "pred = [x in section_names for x in metadata_extended['brain_section_label']]\n",
    "sections = metadata_extended[pred] # this is (num_cells)x21\n",
    "print(np.array(pred).sum())\n",
    "gnames = gene['gene_symbol'].values[:510]\n",
    "pred = [x in gnames for x in adata.var.gene_symbol]\n",
    "gene_filtered = adata.var[pred]\n",
    "asubset = adata[:,gene_filtered.index].to_memory() #this is 4mln x (num_genes)\n",
    "gdata = asubset.to_df()\n",
    "gdata.columns = gnames\n",
    "joined = sections.join(gdata, 'cell_label')\n",
    "# -------------------\n",
    "data = joined[gnames].to_numpy()\n",
    "print('----------\\n DATA SUCCESSFULLY SELECTED\\n----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b540c-7505-42d0-8521-30cb6258e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sublcasses = sections['subclass'].values\n",
    "vals,counts= np.unique(sublcasses, return_counts=True)\n",
    "classes = sections['class'].values\n",
    "listclasses = list(dict.fromkeys(classes))\n",
    "del metadata\n",
    "del metadata_extended\n",
    "del gene_filtered\n",
    "del gdata\n",
    "del sections\n",
    "del pred\n",
    "\n",
    "\n",
    "labels = np.loadtxt(f'{folder}labels.dat') #all dataset label\n",
    "# ------------------------------------------------------------------\n",
    "classes = labels\n",
    "listclasses = np.sort(list(dict.fromkeys(labels)))\n",
    "sizes = np.array([(labels==i).sum() for i in listclasses])\n",
    "xi = np.vstack([data[classes==listclasses[i]][:,:500] for i in np.argsort(sizes)[::-1][:NUM_CLASSES]])\n",
    "ordered_listclasses = listclasses[np.argsort(sizes)[::-1]]\n",
    "label = np.hstack([[ordered_listclasses[i]]*np.sort(sizes)[::-1][i] for i in range(NUM_CLASSES)])\n",
    "label_list = np.array(list(dict.fromkeys(label))).astype(int)\n",
    "#------------------\n",
    "data_classes = np.vstack(xi)\n",
    "subcl =  np.hstack([sublcasses[classes==listclasses[i]] for i in np.argsort(sizes)[::-1][:NUM_CLASSES]])\n",
    "v,c = np.unique(subcl, return_counts=True)\n",
    "with open(f'{folder}SUBCLASSES.dat', 'w') as f:\n",
    "    for line in subcl:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171542e8-dce2-4bca-82a8-a48921441378",
   "metadata": {},
   "source": [
    "# Statistics of raw counts (fig1-sum_cc.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36573c-b476-40ea-a407-1c7a34709835",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of raw counts\n",
    "x = data_classes\n",
    "bins = np.arange(x.min(),x.max()+2,)-0.5\n",
    "HISTO= []\n",
    "for i in trange(500):\n",
    "    histo = np.histogram(x[:,i],bins);\n",
    "    HISTO.append(histo[0]) \n",
    "HISTO = np.array(HISTO)\n",
    "# SAVE\n",
    "np.savetxt(f'{folder_saved}_average_histo_rawc.dat',HISTO.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d378346-7abc-4b44-8bda-d50b728ea304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of raw correlations\n",
    "cc = np.corrcoef(data_classes.T)\n",
    "# SAVE\n",
    "np.savetxt(f'{folder_saved}cc_raw.dat', cc[np.triu_indices(500,1)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d217c-8480-4d0b-97f0-e32d2fba41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution of raw sum (and comparison with suffled case)\n",
    "data=x\n",
    "# ----- shuffle data\n",
    "datas =np.copy(data)\n",
    "all_idx = []\n",
    "for i in trange(len(datas.T)):\n",
    "    np.random.shuffle(datas[:,i]) \n",
    "hh1 = np.histogram(data.sum(1),-0.5+np.arange(data.sum(1).min(), data.sum(1).max()+2) );\n",
    "hh2 = np.histogram(datas.sum(1),-0.5+np.arange(datas.sum(1).min(), datas.sum(1).max()+2));\n",
    "# SAVE\n",
    "np.savetxt(f'{folder_saved}histo_sumraw.dat', np.vstack((hh1[1][1:],hh1[0]/hh1[0].sum())))\n",
    "np.savetxt(f'{folder_saved}histo_sumraw_shuffled.dat', np.vstack((hh2[1][1:],hh2[0]/hh2[0].sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166e440-0b11-47f7-a977-6ec36194a6b5",
   "metadata": {},
   "source": [
    "# Classification accuracy single raw count (acc_1gene.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676c16d-dbb2-44ce-a9ff-eca8e80862a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(f'{folder}data.dat')\n",
    "labels = np.loadtxt(f'{folder}labels.dat')\n",
    "\n",
    "# -------------------- load binary data\n",
    "NUM_CLASSES = 8\n",
    "classes = labels\n",
    "listclasses = np.sort(list(dict.fromkeys(labels)))\n",
    "sizes = np.array([(labels==i).sum() for i in listclasses])\n",
    "xi = np.vstack([data[classes==listclasses[i]][:,:500] for i in np.argsort(sizes)[::-1][:NUM_CLASSES]])\n",
    "ordered_listclasses = listclasses[np.argsort(sizes)[::-1]]\n",
    "label = np.hstack([[ordered_listclasses[i]]*np.sort(sizes)[::-1][i] for i in range(NUM_CLASSES)])\n",
    "label_list = np.array(list(dict.fromkeys(label))).astype(int)\n",
    "label = np.hstack([[ordered_listclasses[i]]*np.sort(sizes)[::-1][i] for i in range(NUM_CLASSES)])\n",
    "# --------------------- distribution of classes\n",
    "pclass=[]\n",
    "for k in label_list:\n",
    "    pclass.append((label ==k).sum()/len(label))\n",
    "pclass = np.array(pclass)\n",
    "np.savetxt(f'{folder_saved}pclass.dat', pclass)\n",
    "\n",
    "### -------------------  plot and save\n",
    "# plt.plot(np.sort(sizes)[::-1], 'o', color='black')\n",
    "# plt.yscale('log')\n",
    "# plt.axhline(500+500*499/2, color= 'grey', ls='--')\n",
    "# plt.xlabel('i')\n",
    "# plt.ylabel('size class i')\n",
    "# np.savetxt(f'{folder_saved}allclass_size.dat', sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b6d6a-f347-4032-91c0-08196b77d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = np.arange(len(xi))\n",
    "y = label\n",
    "\n",
    "ax_shuffled = np.random.choice(ax,len(xi), replace=False)\n",
    "train_size = int(0.8*len(ax_shuffled))\n",
    "\n",
    "xi_train = xi[ax_shuffled][:train_size]\n",
    "xi_test =  xi[ax_shuffled][train_size:]\n",
    "y_train =  y[ax_shuffled][:train_size]\n",
    "y_test =   y[ax_shuffled][train_size:] \n",
    "\n",
    "d=1\n",
    "saved=[]\n",
    "\n",
    "for kk in trange(0,500):\n",
    "    random_genes=[kk]\n",
    "    x_train = xi_train[:,random_genes]\n",
    "    x_test =  xi_test[:,random_genes]\n",
    "    hindi =[]\n",
    "    for k in label_list:\n",
    "        zi = x_train[y_train==k]  \n",
    "        mu = zi.mean(0)\n",
    "        h = -np.arctanh(mu-1e-15)\n",
    "        hindi.append(h)\n",
    "    hindi = np.array(hindi)\n",
    "\n",
    "    prob_xgiveny = (np.exp(-hindi[:,None,:]*x_test[None])/(np.exp(-hindi)+np.exp(hindi))[:,None,:]).prod(-1)\n",
    "    prob_ygivenx = prob_xgiveny*pclass[:,None]\n",
    "    prob_ygivenx = prob_ygivenx/prob_ygivenx.sum(0)\n",
    "    y_pred_condind = label_list[np.argmax(prob_ygivenx,0)]\n",
    "    acc = (y_pred_condind == y_test).sum()/len(y_test)\n",
    "    saved.append((kk,acc))\n",
    "\n",
    "svd = np.array(saved)\n",
    "ordered_variables= (svd[:,0][np.argsort(svd[:,1])[::-1]]).astype(int)\n",
    "svd_all = np.vstack((svd, [0,(y_test==1).sum()/len(y_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b1c99-21e0-42ad-887e-a0482cc91ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select groups of genes to use at each size d (vary d)\n",
    "d = 50\n",
    "# -----------------------------------------------------\n",
    "saved =[]\n",
    "for boundary_selection in [d+50, -d-50, 500]:\n",
    "    for kk in trange(50):\n",
    "        random_genes=np.random.choice(ordered_variables[:boundary_selection], d, replace=False)\n",
    "        x_train = xi_train[:,random_genes]\n",
    "        x_test =  xi_test[:,random_genes]\n",
    "        hindi =[]\n",
    "        for k in label_list:\n",
    "            zi = x_train[y_train==k]  \n",
    "            mu = zi.mean(0)\n",
    "            h = -np.arctanh(mu-1e-15)\n",
    "            hindi.append(h)\n",
    "        hindi = np.array(hindi)\n",
    "        prob_xgiveny = (np.exp(-hindi[:,None,:]*x_test[None])/(np.exp(-hindi)+np.exp(hindi))[:,None,:]).prod(-1)\n",
    "        prob_ygivenx = prob_xgiveny*pclass[:,None]\n",
    "        prob_ygivenx = prob_ygivenx/prob_ygivenx.sum(0)\n",
    "        y_pred_condind = label_list[np.argmax(prob_ygivenx,0)]\n",
    "        acc = (y_pred_condind == y_test).sum()/len(y_test)\n",
    "        saved.append(np.hstack((random_genes, acc)))\n",
    "        \n",
    "svd = np.array(saved)\n",
    "plt.plot((svd[:,-1]),'o')\n",
    "plt.show()\n",
    "random_genes = svd[np.argsort(svd[:,-1])[::-1]][0][:-1].astype(int)\n",
    "np.savetxt(f'{folder}_genes_{d}.dat',random_genes)\n",
    "print(random_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55103c9a-65da-49d1-94c2-8a13d71e622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of raw data\n",
    "\n",
    "acc_nat =[]\n",
    "for d in [3,5,10,15,30,50,100,200,500]:\n",
    "    random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "    #----------------------------\n",
    "    x_train = x_train_all[:,random_genes]\n",
    "    x_test =  x_test_all[:,random_genes]\n",
    "    # ----------------\n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(), \n",
    "                        random_state=1, batch_size=64, learning_rate_init =1e-4,\n",
    "                       verbose=True, max_iter=1000, tol=1e-3)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = (y_pred==y_test).sum()/len(y_pred)\n",
    "    #--------------\n",
    "    print(d, accuracy)  \n",
    "    acc_nat.append((d, accuracy))    \n",
    "    np.savetxt(f'{folder_saved}acc_nat.dat', np.array(acc_nat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fda3e4-3271-494d-a0f3-624ab70b3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a NN to find the accuracy\n",
    "\n",
    "acc_n =[]\n",
    "for d in [2,3,5,10,30,50,100,200,500]:\n",
    "    random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "    #----------------------------\n",
    "    x_train = x_train_all[:,random_genes]\n",
    "    x_test =  x_test_all[:,random_genes]\n",
    "    # ----------------\n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100), \n",
    "                        random_state=1, batch_size=64, learning_rate_init =1e-4,\n",
    "                       verbose=True, max_iter=1000, tol=1e-3)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = (y_pred==y_test).sum()/len(y_pred)\n",
    "    #--------------\n",
    "    print(d, accuracy)  \n",
    "    acc_n.append((d, accuracy))    \n",
    "    np.savetxt(f'{folder_saved}acc_n.dat', np.array(acc_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27936b08-433e-4464-91a9-6c154e7a62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average accuracy with binary data\n",
    "x = xi\n",
    "y = label\n",
    "#----------------------------\n",
    "ax = np.arange(len(x))\n",
    "ax_shuffled = np.random.choice(ax,500000, replace=False)\n",
    "train_size = int(0.8*len(ax_shuffled))    \n",
    "#----------------------------\n",
    "x_train_all = x[ax_shuffled][:train_size]\n",
    "x_test_all =  x[ax_shuffled][train_size:]\n",
    "y_train = y[ax_shuffled][:train_size]\n",
    "y_test =  y[ax_shuffled][train_size:]\n",
    "\n",
    "filepath = f'{folder_saved}acc_bin_AVG_{NUM_CLASSES}.dat'\n",
    "\n",
    "if os.path.isfile(filepath)== False:\n",
    "    acc_nat_avg = []\n",
    "    np.savetxt(filepath, np.array(acc_nat_avg)) \n",
    "    print('new file generated')\n",
    "acc_nat_avg = np.loadtxt(filepath)\n",
    "acc_nat_avg = [i for i in acc_nat_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892794be-537a-48b0-852f-f8f06db6a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_nat_avg = [i for i in acc_nat_avg]\n",
    "for d in [1,2,3,5,10,15,30,50,100,200,500]:\n",
    "    acc=[]\n",
    "    nrepetitions=20\n",
    "    if d==500: nrepetitions=1\n",
    "    for _ in trange(nrepetitions):\n",
    "        random_genes = np.random.choice(500,d,replace=False)\n",
    "        #----------------------------\n",
    "        x_train = x_train_all[:,random_genes]\n",
    "        x_test =  x_test_all[:,random_genes]\n",
    "        # ----------------\n",
    "        clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100), \n",
    "                            random_state=1, batch_size=64, learning_rate_init =1e-3,\n",
    "                           verbose=True, max_iter=1000, tol=1e-3)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        accuracy = (y_pred==y_test).sum()/len(y_pred)\n",
    "        acc.append(accuracy)\n",
    "        print(random_genes, acc)\n",
    "        \n",
    "    acc = np.array(acc)\n",
    "    #--------------\n",
    "    print(d, acc.mean(), acc.std())  \n",
    "    acc_nat_avg.append((d, acc.mean(), acc.std()))   \n",
    "    np.savetxt(filepath, np.array(acc_nat_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea372d47-a626-443e-8bc2-ad8825fbf8de",
   "metadata": {},
   "source": [
    "# Statistics of binarized data (binarization.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760829e5-28d1-4e37-8047-3ee7f618b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of gene expression given zero probability\n",
    "prob_0 = (data_classes==0).sum(0)/len(data_classes)\n",
    "\n",
    "mean_not0 = []\n",
    "for i in trange(len(data_classes.T)):\n",
    "    mean_not0.append(data_classes[:,i][data_classes[:,i]!=0].mean())\n",
    "mean_not0 = np.array(mean_not0)\n",
    "\n",
    "p0_means = np.vstack((prob_0, mean_not0, data_classes.mean(0))).T\n",
    "np.savetxt(f'{folder_saved}p0_means.dat', p0_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c3770-2e06-4a98-9299-9bf5cd6a5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load binarized data\n",
    "data = np.loadtxt(f'{folder}data.dat')\n",
    "labels = np.loadtxt(f'{folder}labels.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85989a21-c51a-4c8e-91d4-87e35bcacce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare binary and raw sums\n",
    "z_classes = data\n",
    "cells = np.random.choice(range(len(z_classes)), 10000, replace=False)\n",
    "sum_zx = np.vstack((z_classes.sum(1)[cells], data_classes.sum(1)[cells]))\n",
    "np.savetxt(f'{folder_saved}sum_zx.dat', sum_zx.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b6718-0f3b-4b3d-b5d2-3ea383977968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute binary correlations\n",
    "cc_z = np.corrcoef(z_classes.T)\n",
    "np.savetxt(f'{folder_saved}cc_bin.dat', cc_z[np.triu_indices(500,1)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f16f45-6b6a-4d73-9f9d-816d2d63469e",
   "metadata": {},
   "source": [
    "# Convergence of Inverse Ising method (convergence_{d}_{label}.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7785d5-b1b4-4f34-a4ff-5d6ffc807152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- generate f_data for the needed d\n",
    "d=500\n",
    "random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "z = xi[:,random_genes]\n",
    "#---------------- save f_data\n",
    "z1=z\n",
    "mu = z1.mean(0)\n",
    "mu2 = (np.cov(z1.T) + mu[:,None]*mu[None,:])[np.triu_indices(d,1)]\n",
    "f_data = np.hstack((mu,mu2))\n",
    "np.savetxt(f'{folder}f_data_{d}_all.dat', f_data)\n",
    "\n",
    "### -------- save a subset for test\n",
    "z = xi[:,random_genes]\n",
    "random_cells = np.random.choice(len(z), 50000, replace=False)\n",
    "z_selection = z[random_cells]\n",
    "label_selection = label[random_cells]\n",
    "np.savetxt(f'{folder}_datasel2_{d}.dat',z_selection)\n",
    "np.savetxt(f'{folder}_datasel2_label_{d}.dat',label_selection)\n",
    "\n",
    "## ------------------------  classes\n",
    "for i in range(len(label_list)):\n",
    "    z1 = z[label==label_list[i]]\n",
    "    mu = z1.mean(0)\n",
    "    mu2 = (np.cov(z1.T) + mu[:,None]*mu[None,:])[np.triu_indices(d,1)]\n",
    "    f_data = np.hstack((mu,mu2))\n",
    "\n",
    "    z1s = np.copy(z1)\n",
    "    for ii in trange(len(z1s.T)):\n",
    "        np.random.shuffle(z1s[:,ii])\n",
    "    histo = plt.hist(z1.sum(1),np.arange(-d,d,2));\n",
    "    histo2 = plt.hist(z1s.sum(1),np.arange(-d,d,2));\n",
    "\n",
    "    np.savetxt(f'{folder}f_data_{d}_{label_list[i]}.dat', f_data)\n",
    "    np.savetxt(f'{folder}_histosum_{d}_{label_list[i]}.dat',histo[0])\n",
    "    np.savetxt(f'{folder}_histosums_{d}_{label_list[i]}.dat',histo2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e551283-df91-40ad-b365-c5a358e46f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------ MOMENTS\n",
    "z = xi[:,random_genes]\n",
    "deltadata = z-f_data[:d]\n",
    "m2=[]\n",
    "m3=[]\n",
    "m4=[]\n",
    "for _ in trange(2000):\n",
    "    i,j,k,l = np.random.choice(10,4, replace=False)\n",
    "    m2.append(((deltadata[:, [i,j]]).prod(1).mean(),  i,j))\n",
    "    m3.append(((deltadata[:, [i,j,k]]).prod(1).mean(), i,j,k))\n",
    "    m4.append(((deltadata[:, [i,j,k,l]]).prod(1).mean(),i,j,k,l))\n",
    "m2=np.array(m2)\n",
    "m3=np.array(m3)    \n",
    "m4=np.array(m4) \n",
    "np.savetxt(f'{folder}_m2_{d}.dat',m2)\n",
    "np.savetxt(f'{folder}_m3_{d}.dat',m3)\n",
    "np.savetxt(f'{folder}_m4_{d}.dat',m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1d423-9a70-4794-8f35-2839c55cb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----------------------- moments in classes\n",
    "z = xi[:,random_genes]\n",
    "for i in range(len(label_list)):\n",
    "    z1 = z[label==label_list[i]]\n",
    "    mu = z1.mean(0)\n",
    "    mu2 = (np.cov(z1.T) + mu[:,None]*mu[None,:])[np.triu_indices(d,1)]\n",
    "    f_data = np.hstack((mu,mu2))\n",
    "    \n",
    "    deltadata = z1-f_data[:d]\n",
    "    m2=[]\n",
    "    m3=[]\n",
    "    m4=[]\n",
    "    for _ in trange(2000):\n",
    "        ii,j,k,l = np.random.choice(10,4, replace=False)\n",
    "        m2.append(((deltadata[:, [ii,j]]).prod(1).mean(),  ii,j))\n",
    "        m3.append(((deltadata[:, [ii,j,k]]).prod(1).mean(), ii,j,k))\n",
    "        m4.append(((deltadata[:, [ii,j,k,l]]).prod(1).mean(),ii,j,k,l))\n",
    "    m2=np.array(m2)\n",
    "    m3=np.array(m3)    \n",
    "    m4=np.array(m4) \n",
    "    np.savetxt(f'{folder}_m2_{d}_{label_list[i]}.dat',m2)\n",
    "    np.savetxt(f'{folder}_m3_{d}_{label_list[i]}.dat',m3)\n",
    "    np.savetxt(f'{folder}_m4_{d}_{label_list[i]}.dat',m4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159b2ad-ba25-4c21-a985-17cd17075cae",
   "metadata": {},
   "source": [
    "# Check Higher order statistics (ising_pred_{d}_{label}.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d7de0-4f3a-4bf1-9d05-f74ba1e791b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------ save heff\n",
    "\n",
    "d=100\n",
    "label='all'\n",
    "\n",
    "q = np.loadtxt(f'{folder_bin}q_{d}_{label}.dat')\n",
    "random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "zsel= np.loadtxt(f'{folder}_datasel2_500.dat')[:,random_genes]\n",
    "\n",
    "h,J = get_hJ(q,d)\n",
    "h=-h/d\n",
    "J=-J/d ##this is already divided by 2\n",
    "\n",
    "heff_all =[]\n",
    "val_all =[]\n",
    "\n",
    "for idx in trange(d):\n",
    "    heff_0 = h[idx] + (J[idx]*zsel).sum(-1)\n",
    "    val_0 = zsel[:,idx]\n",
    "    heff_all.append(heff_0)\n",
    "    val_all.append(val_0)\n",
    "heff_all = np.hstack(heff_all)\n",
    "val_all = np.hstack(val_all)\n",
    "px = heff_all[np.argsort(heff_all)]\n",
    "py = val_all[np.argsort(heff_all)]\n",
    "\n",
    "\n",
    "values = np.linspace(px.min(),px.max(),20)\n",
    "# ---------------------------------------------------\n",
    "valuesy=[]\n",
    "valuesyerr=[]\n",
    "valuesx=[]\n",
    "valuesxerr=[]\n",
    "for i in trange(len(values)-1):\n",
    "    ll=len(py[(px>values[i])*(px<values[i+1])])\n",
    "    valuesx.append(px[(px>values[i])*(px<values[i+1])].mean())\n",
    "    valuesy.append(py[(px>values[i])*(px<values[i+1])].mean())\n",
    "    valuesyerr.append(py[(px>values[i])*(px<values[i+1])].std()/np.sqrt(ll)) \n",
    "    valuesxerr.append(px[(px>values[i])*(px<values[i+1])].std()/np.sqrt(ll)) \n",
    "valuesy = np.array(valuesy)\n",
    "valuesx = np.array(valuesx)\n",
    "valuesyerr=np.array(valuesyerr)\n",
    "valuesxerr=np.array(valuesxerr)\n",
    "\n",
    "tobesaved = np.vstack((valuesx, valuesy,valuesxerr,valuesyerr))\n",
    "np.savetxt(f'{folder_bin}_heff_{d}_{label}.dat', tobesaved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f8ed9-8d97-48d4-8c44-4b7f09753a85",
   "metadata": {},
   "source": [
    "# Specific heat - model vs independent case (specific_heat.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614debf8-99de-4f6a-81a2-a4dbe3f42c60",
   "metadata": {},
   "source": [
    "See find_ising.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac8d21-30dd-4be9-80c1-178733fd0223",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Energy basins (basins_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f19f3f-1d4b-47ab-85b0-6f4a7c745e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save subset for energy basins (just once)\n",
    "# indices = np.random.choice(range(len(data)), 500000, replace=False)\n",
    "# np.savetxt(f'{folder}b_classes.dat',np.array(classes)[indices])\n",
    "# np.savetxt(f'{folder}b_subclasses.dat',np.array(subclasses)[indices])\n",
    "# np.savetxt(f'{folder}b_indices.dat',indices)\n",
    "# np.savetxt(f'{folder}b_data.dat', x[indices].astype(int))\n",
    "\n",
    "# ### -------- save a subset for test (just once)\n",
    "# z = xi[:,random_genes]\n",
    "# random_cells = np.random.choice(len(z), 50000, replace=False)\n",
    "# z_selection = z[random_cells]\n",
    "# label_selection = label[random_cells]\n",
    "# np.savetxt(f'{folder}_datasel2_{d}.dat',z_selection)\n",
    "# np.savetxt(f'{folder}_datasel2_label_{d}.dat',label_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267483ab-35f1-421b-981d-97bfa73effd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mimima\n",
    "d = 50\n",
    "label='all'\n",
    "q = np.loadtxt(f'{folder_bin}q_{d}_{label}.dat')\n",
    "\n",
    "zsel_all = np.loadtxt(f'{folder}_datasel2_500.dat')[:]\n",
    "lsel = np.loadtxt(f'{folder}_datasel2_label_500.dat')[:]\n",
    "random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "zsel = zsel_all[:,random_genes]\n",
    "print(len(zsel), d)\n",
    "\n",
    "labellist = np.array(list(dict.fromkeys(lsel)))\n",
    "labellist=np.sort(labellist)\n",
    "print(labellist)\n",
    "\n",
    "if os.path.isfile(f'{folder_bin}minimaN_{d}_{label}.dat'):\n",
    "    minima = list(np.loadtxt(f'{folder_bin}minimaN_{d}_{label}.dat') )\n",
    "else: minima=[]\n",
    "len(minima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551c6ea-ce43-46f1-9acc-0aad4ae7ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(len(minima), len(zsel)):\n",
    "    cell= zsel[i]\n",
    "    with open(f'{folder_bin}minimaN_{d}_{label}.dat','ab') as f:\n",
    "        f.write(b'\\n')\n",
    "        np.savetxt(f, [get_minimum(cell,q)], fmt='%d', delimiter=' ', newline='')\n",
    "        f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b3032-6e2c-4b04-8e89-7cf8a610ccad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Energy landscape (energy_landscape_{d}.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fa212-e3fb-4772-9305-8a5045736e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------- save first largest minima\n",
    "d=500\n",
    "label='all'\n",
    "minima = np.loadtxt(f'{folder_bin}minimaN_{d}_{label}.dat')\n",
    "lst, size = np.unique(minima, return_counts=True,axis=0)\n",
    "lst = lst[np.argsort(size)[::-1]]\n",
    "size = size[np.argsort(size)[::-1]]\n",
    "np.savetxt(f'{folder_bin}first_minima_{d}_{all}.dat',lst[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394cfe6-e081-4bfd-b3f2-8f7f088f5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------- energy histogram\n",
    "d = 500\n",
    "label='all'\n",
    "q = np.loadtxt(f'{folder_bin}q_{d}_{label}.dat')\n",
    "h,J = get_hJ(q,d)\n",
    "\n",
    "zsel_all = np.loadtxt(f'{folder}_datasel2_500.dat')[:]\n",
    "lsel = np.loadtxt(f'{folder}_datasel2_label_500.dat')[:]\n",
    "\n",
    "random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "zsel = zsel_all[:,random_genes]\n",
    "print(len(zsel), d)\n",
    "\n",
    "basin = np.loadtxt(f'{folder_bin}basinN_{d}_{label}.dat').astype(int)\n",
    "\n",
    "class_of_basn = np.array([mode(lsel[basin==i]) for i in range(1+int(basin.max()))])\n",
    "lsel_pred = np.zeros(len(lsel))\n",
    "for i in range(len(lsel_pred)):\n",
    "    lsel_pred[i] = class_of_basn[basin[i]]\n",
    "    \n",
    "for k in [0,1,2,3,4]:\n",
    "    energy_cells=[]\n",
    "    for i in trange(1000):\n",
    "        energy_cells.append(np.array(get_H(zsel[basin==k][i*500:(i+1)*500],J,h)))\n",
    "    energy_cells = np.hstack(energy_cells)\n",
    "    histo = np.histogram(energy_cells,100, density=True);\n",
    "    plt.plot(histo[1][1:], histo[0])\n",
    "    \n",
    "    np.savetxt(f'{folder_bin}energy_basin_{k}_d{d}_{label}.dat', energy_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fa1ea-4bc1-4e03-88d6-0a222b35a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------ Energy barriers\n",
    "d=500\n",
    "label='all'\n",
    "q = np.loadtxt(f'{folder_bin}q_{d}_{label}.dat')\n",
    "h,J = get_hJ(q,d)\n",
    "largest_minima = np.loadtxt(f'{folder_bin}first_minima_{d}_{all}.dat')\n",
    "en_minima = get_H(largest_minima,J,h)\n",
    "\n",
    "i=4\n",
    "final_state=[]\n",
    "final_nstep=[]\n",
    "final_barrier=[]\n",
    "l0=0\n",
    "\n",
    "# final_state=list(np.loadtxt(f'{folder_bin}barriers_finalstate_{d}_{i}_.dat'))\n",
    "# final_nstep= list(np.loadtxt(f'{folder_bin}barriers_final_nstep_{d}_{i}_.dat'))\n",
    "# final_barrier=list(np.loadtxt(f'{folder_bin}barriers_final_barrier_{d}_{i}_.dat'))\n",
    "\n",
    "l0 = len(final_barrier)\n",
    "print(l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d6734-5ed3-4f0c-9211-d00132291126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500-l0):\n",
    "    x0 = np.copy(largest_minima[i])\n",
    "    x = np.copy(x0)[None,:]\n",
    "    Hx = en_minima[i]\n",
    "    \n",
    "    numsteps=0\n",
    "    hmax = Hx\n",
    "    for _ in trange(10000):\n",
    "        for _ in range(1):\n",
    "            x, Hx = one_step_MC(x, h,J,Hx)\n",
    "            numsteps+=1\n",
    "            if Hx>hmax: hmax=Hx\n",
    "        if (get_minimum(x[0],q) == x0).prod()!= 1:\n",
    "            if len((np.where((get_minimum(x[0],q) == largest_minima).prod(1)==1)[0]>0)):\n",
    "                final_state.append((np.where((get_minimum(x[0],q) == largest_minima).prod(1)==1)[0][0]))\n",
    "            final_nstep.append(numsteps)\n",
    "            final_barrier.append(hmax.numpy()[0])  \n",
    "            \n",
    "            print(len(final_state),numsteps)\n",
    "            \n",
    "            np.savetxt(f'{folder_bin}barriers_finalstate_{d}_{i}_.dat', np.array(final_state))\n",
    "            np.savetxt(f'{folder_bin}barriers_final_nstep_{d}_{i}_.dat', np.array(final_nstep))\n",
    "            np.savetxt(f'{folder_bin}barriers_final_barrier_{d}_{i}_.dat', np.array(final_barrier))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed08b17-6ddf-428e-925a-4e2bdbe3bb67",
   "metadata": {},
   "source": [
    "# Confusion matrix (confusion_1ising.pdf and NN_vs_mixIsing.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e567f-5247-48d0-8ca4-7e08789770de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_{ds[i]}_{label}.dat'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b6b67c4-12a9-485b-ac5a-9e60e4d3347c",
   "metadata": {},
   "source": [
    "data = np.loadtxt(f'{folder}data.dat')\n",
    "labels = np.loadtxt(f'{folder}labels.dat')\n",
    "\n",
    "NUM_CLASSES = 8\n",
    "# ------------------------------------------------------------------\n",
    "classes = labels\n",
    "listclasses = np.sort(list(dict.fromkeys(labels)))\n",
    "sizes = np.array([(labels==i).sum() for i in listclasses])\n",
    "xi = np.vstack([data[classes==listclasses[i]][:,:500] for i in np.argsort(sizes)[::-1][:NUM_CLASSES]])\n",
    "ordered_listclasses = listclasses[np.argsort(sizes)[::-1]]\n",
    "label = np.hstack([[ordered_listclasses[i]]*np.sort(sizes)[::-1][i] for i in range(NUM_CLASSES)])\n",
    "label_list = np.array(list(dict.fromkeys(label))).astype(int)\n",
    "print(xi.shape, label_list)\n",
    "\n",
    "label = np.hstack([[ordered_listclasses[i]]*np.sort(sizes)[::-1][i] for i in range(NUM_CLASSES)])\n",
    "\n",
    "pclass=[]\n",
    "for k in label_list:\n",
    "    pclass.append((label ==k).sum()/len(label))\n",
    "pclass = np.array(pclass)\n",
    "\n",
    "accuracy_shuffled = max(pclass)\n",
    "\n",
    "\n",
    "x = xi\n",
    "y = label\n",
    "#----------------------------\n",
    "ax = np.arange(len(x))\n",
    "ax_shuffled = np.random.choice(ax,len(ax), replace=False)\n",
    "train_size = int(0.8*len(ax_shuffled))    \n",
    "#----------------------------\n",
    "x_train_all = x[ax_shuffled][:train_size]\n",
    "x_test_all =  x[ax_shuffled][train_size:]\n",
    "y_train = y[ax_shuffled][:train_size]\n",
    "y_test =  y[ax_shuffled][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc1935-901f-4b45-842c-e595a41ff0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------ Confusion matrix for raw accuracy NN\n",
    "\n",
    "d=50\n",
    "random_genes = np.loadtxt(f'{folder}_genes_{d}.dat').astype(int)\n",
    "#----------------------------\n",
    "x_train = x_train_all[:,random_genes]\n",
    "x_test =  x_test_all[:,random_genes]\n",
    "# ----------------\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100), \n",
    "                    random_state=1, batch_size=64, learning_rate_init =1e-2,\n",
    "                   verbose=True, max_iter=10, tol=1e-3)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy = (y_pred==y_test).sum()/len(y_pred)\n",
    "\n",
    "# _____________________________________\n",
    "labellist= label_list\n",
    "confusion = np.zeros((len(labellist), len(labellist)))\n",
    "for ik in range(len(labellist)):\n",
    "    k = labellist[ik]\n",
    "    for ikk in range(len(labellist)):\n",
    "        kk=labellist[ikk]\n",
    "        confusion[ik,ikk] = ((y_pred[y_test==k]==kk).sum()/ len(y_pred[y_test==k]))\n",
    "        \n",
    "np.savetxt(f'{folder}confusion_natNN_{d}.dat',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55462e-d879-4b1e-98d6-81186abb299e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mix Ising Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41ef18-197b-41c6-8195-3844ba97a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ratio(x,q1,q2, NumSteps=1000,StepsTherm=10):\n",
    "    d = len(x[0])\n",
    "    h1,J1 = get_hJ(q1,d)\n",
    "    h1 = tf.convert_to_tensor(h1,dtype=tf.float64)\n",
    "    J1= tf.convert_to_tensor(J1,dtype=tf.float64)\n",
    "    \n",
    "    h2,J2 = get_hJ(q2,d)\n",
    "    h2 = tf.convert_to_tensor(h2,dtype=tf.float64)\n",
    "    J2= tf.convert_to_tensor(J2,dtype=tf.float64)\n",
    "\n",
    "    num_part,d = x.shape\n",
    "    num_par = int(d+ d*(d-1)/2)\n",
    "\n",
    "    ratio = 0\n",
    "    H2 = get_H(x,J2,h2)\n",
    "    for n in trange(NumSteps):\n",
    "        for _ in tf.range(StepsTherm):\n",
    "            x,H2 = one_step_MC(x, h2,J2,H2)\n",
    "        H1 = get_H(x,J1,h1)\n",
    "        ratio_partial = tf.reduce_mean(tf.exp(-(H1-H2)))\n",
    "        ratio = (n*ratio + ratio_partial)/(n+1)  \n",
    "        \n",
    "    return ratio.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb83c9-7355-4678-b7da-96f2f418ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12935464-d718-4aea-875a-6ea42005091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes =np.array([1,30,31,33,9,29,2,28])\n",
    "N_CLASSES = len(list_classes)\n",
    "\n",
    "## ----- ---Load f_data and q for each class\n",
    "label ='all'\n",
    "f_data_all = np.loadtxt(f'{folder_bin}f_data_{d}_{label}.dat')\n",
    "f_data_i=[]\n",
    "for label in list_classes:\n",
    "    f_data_i.append(np.loadtxt(f'{folder_bin}f_data_{d}_{label}.dat'))\n",
    "f_data_i = np.vstack((f_data_i))\n",
    "# ---------------------------------\n",
    "label ='all'\n",
    "q_all = np.loadtxt(f'{folder_bin}q_{d}_{label}.dat')\n",
    "q_i=[]\n",
    "for label in list_classes:\n",
    "    q_i.append(np.loadtxt(f'{folder_bin}q_{d}_{label}.dat'))\n",
    "q_i = np.vstack((q_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b85a9-2afe-4599-b8a7-38557423c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Thermalize points at q_all\n",
    "q = q_all\n",
    "h,J = get_hJ(q,d)\n",
    "h = tf.convert_to_tensor(h,dtype=tf.float64)\n",
    "J= tf.convert_to_tensor(J,dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4003aaa-3c2c-4197-9e7b-11bb90f76b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 8000\n",
    "x = np.random.choice([-1,1], (n_samples,d))\n",
    "x = tf.convert_to_tensor(x,dtype=tf.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd7218-c09b-4b75-820c-5e9888a267ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### thermalize x to the q ###\n",
    "n_thermalize = 50000\n",
    "x, energy_history = thermalize(x,J,h, NumSteps=n_thermalize , crange =trange)\n",
    "plt.plot(energy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e857c0-2aef-430e-8156-680071d88d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(x[0])\n",
    "energy_x_class =[]\n",
    "\n",
    "for i in range(N_CLASSES):\n",
    "    q = q_i[i]\n",
    "    h,J = get_hJ(q,d)\n",
    "    H1 = get_H(x,J,h)\n",
    "    energy_x_class.append(H1)\n",
    "energy_x_class = np.array(energy_x_class)\n",
    "\n",
    "h2,J2 = get_hJ(q_all,d)\n",
    "h2 = tf.convert_to_tensor(h2,dtype=tf.float64)\n",
    "J2= tf.convert_to_tensor(J2,dtype=tf.float64)\n",
    "H2 = get_H(x,J2,h2)\n",
    "\n",
    "num_part,d = x.shape\n",
    "num_par = int(d+ d*(d-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6739089-5ece-4ef2-8394-7ffdf76239e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = np.zeros(N_CLASSES)\n",
    "n=0\n",
    "ratio_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6607c-a9fe-4f44-9342-45e7514e0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumSteps=1000\n",
    "StepsTherm=50\n",
    "\n",
    "for _ in trange(NumSteps):\n",
    "    for _ in tf.range(StepsTherm):\n",
    "        x,H2 = one_step_MC(x, h2,J2,H2)    \n",
    "    energy_x_class =[]\n",
    "    for i in range(N_CLASSES):\n",
    "        q = q_i[i]\n",
    "        h,J = get_hJ(q,d)\n",
    "        H1 = get_H(x,J,h)\n",
    "        energy_x_class.append(H1)\n",
    "    energy_x_class = np.array(energy_x_class) \n",
    "\n",
    "    ratio_partial = tf.reduce_mean(tf.exp(-(energy_x_class-H2)),1)\n",
    "    ratio = (n*ratio + ratio_partial)/(n+1)  \n",
    "    ratio_history.append(np.array(ratio).mean())\n",
    "    n+=1\n",
    "ratio = ratio.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498534a9-4f16-4f72-aa8e-72877a46854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ratio_history[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b506-c47a-4c6f-ac66-4c63f0d38483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Pclass from labels\n",
    "pclass_ = np.loadtxt(f'{folder_bin}pclass.dat')\n",
    "if (pclass_[:,0].astype(int) == list_classes).prod() ==1:\n",
    "    pclass = pclass_[:,1]\n",
    "    print('pclass correctly loaded')\n",
    "else: \n",
    "    print('there is a problem')\n",
    "    print(pclass_[:,0].astype(int), list_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34c6ec-9677-421f-ac74-bc2d7b756227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x,y to test the classifier\n",
    "x = np.loadtxt(f'{folder_bin}_datasel_{d}.dat')[:]\n",
    "y = np.loadtxt(f'{folder_bin}_datasel_label_{d}.dat')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab381887-50fe-4d4f-9ca5-c76cc5838ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find H(x|y)\n",
    "energy_x_class =[]\n",
    "for i in trange(N_CLASSES):\n",
    "    q = q_i[i]\n",
    "    h,J = get_hJ(q,d)  \n",
    "    H1=[]\n",
    "    for ii in range(int(len(x)/100)+1):\n",
    "        H1.append(get_H(x[ii*100:(1+ii)*100],J,h))\n",
    "    H1 = np.hstack(H1)\n",
    "    energy_x_class.append(H1)\n",
    "energy_x_class = np.array(energy_x_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82534bc3-7b7d-44aa-8178-f67c59dbf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find P(y|x) propto exp(-H_class)P_class/r_class\n",
    "prob_y_given_x_nn = np.exp(-energy_x_class)*pclass[:,None]/ratio[:,None]\n",
    "prob_y_given_x = prob_y_given_x_nn/prob_y_given_x_nn.sum(0)\n",
    "np.savetxt(f'{folder_bin}pyx_{d}.txt',prob_y_given_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d2e8f-0dcf-48f4-b7c5-3f47927d0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y_given_x = np.loadtxt(f'{folder_bin}pyx_{d}.txt')\n",
    "y_pred_temp = np.argmax(prob_y_given_x,0)\n",
    "y_pred = np.copy(y_pred_temp)\n",
    "for i in range(N_CLASSES):\n",
    "    y_pred[y_pred_temp==i]=list_classes[i]\n",
    "acc = (y_pred == y).sum()/len(y)\n",
    "print(d,acc)\n",
    "\n",
    "label_list = list_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deaf676-a807-451b-b7ab-c7317248e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_perclass =np.array([ (y_pred[y==i] == y[y==i]).sum()/len(y[y==i]) for i in label_list])\n",
    "acc_perclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97b976-c68b-4d3e-913c-b2b51fe01c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.zeros((len(label_list), len(label_list)))\n",
    "for ik in range(len(label_list)):\n",
    "    k = label_list[ik]\n",
    "    for ikk in range(len(label_list)):\n",
    "        kk=label_list[ikk]\n",
    "        confusion[ik,ikk] = ((y_pred[y==k]==kk).sum()/ len(y_pred[y==k]))\n",
    "np.savetxt(f'{folder}confusion_mixising_{d}.dat',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34792e-8822-4732-9fdd-0cee9b034fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# entropies_1ising.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce5a5e-e352-4685-ae07-44f0119080f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2,3,5,10,15,30,50,100,200,500]\n",
    "ACC =[]\n",
    "MI_all=[]\n",
    "lsel = np.loadtxt(f'{folder}_datasel2_label_500.dat')\n",
    "label='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84577d49-4935-4c43-9160-e367b4aaff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sizes:\n",
    "    minima = np.loadtxt(f'{folder_bin}minimaN_{d}_{label}.dat')\n",
    "\n",
    "    lst, size = np.unique(minima, return_counts=True,axis=0)\n",
    "    lst = lst[np.argsort(size)[::-1]]\n",
    "    size = size[np.argsort(size)[::-1]]\n",
    "    # -----\n",
    "    basin =[]\n",
    "    for i in trange(len(minima)):\n",
    "        basin.append(np.where((minima[i] == lst).prod(1)==1)[0][0])\n",
    "    basin = np.array(basin)\n",
    "    \n",
    "    np.savetxt(f'{folder_bin}basinN_{d}_{label}.dat',basin)\n",
    "    \n",
    "    class_of_basn = np.array([mode(lsel[basin==i]) for i in range(1+int(basin.max()))])\n",
    "    lsel_pred = np.zeros(len(lsel))\n",
    "    for i in range(len(lsel_pred)):\n",
    "        lsel_pred[i] = class_of_basn[basin[i]]\n",
    "    ## complessive accuracy\n",
    "    accuracy = (lsel_pred == lsel).sum()/len(lsel)\n",
    "    print(accuracy)\n",
    "    ACC.append((d,accuracy))\n",
    "    \n",
    "    # --------------------\n",
    "    threshold = 0.001*len(minima)\n",
    "    imax = (size>threshold).sum()\n",
    "    print(threshold, imax)\n",
    "    labellist = np.array(list(dict.fromkeys(lsel)))\n",
    "    labellist=np.sort(labellist)\n",
    "    \n",
    "    entropies =[]\n",
    "    prob_table =[]\n",
    "    for ii in trange(imax):\n",
    "        prob_type_given_basin = np.array([(lsel[basin==ii]==i).sum()/len(lsel[basin==ii]) for i in labellist])\n",
    "        entropy = -(prob_type_given_basin[prob_type_given_basin!=0]*np.log(prob_type_given_basin[prob_type_given_basin!=0])).sum()\n",
    "        entropies.append(entropy)\n",
    "        prob_table.append(prob_type_given_basin)\n",
    "    entropies=np.array(entropies)\n",
    "    prob_table=np.array(prob_table)\n",
    "    np.savetxt(f'{folder_bin}prob_table_{d}_{label}.dat', prob_table)\n",
    "    \n",
    "    prob_basin = np.array([(basin==i).sum() for i in range(len(entropies))])\n",
    "    prob_basin=prob_basin/prob_basin.sum()\n",
    "    cond_entropy = (prob_basin*entropies).sum()\n",
    "    lsel_large = lsel[basin<imax]\n",
    "    prob_label = np.array([(lsel_large ==i).sum()/len(lsel_large) for i in labellist])\n",
    "    entro_label = -(prob_label[prob_label!=0] * np.log(prob_label[prob_label!=0])).sum()\n",
    "    MI_label_basin = entro_label - cond_entropy\n",
    "    MI_all.append((d, MI_label_basin))\n",
    "ACC = np.array(ACC)\n",
    "MI_all = np.array(MI_all)\n",
    "\n",
    "np.savetxt(f'{folder_bin}new_ACC_1ising_{label}.dat',ACC)\n",
    "np.savetxt(f'{folder_bin}new_MI_1ising_{label}.dat',MI_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffc462-babb-4b07-a50b-c65051091b3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Other Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaade7d-7fb1-4d6c-8452-6b58b34df85f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "NOTE: use find_ising.ipynb or find_ising.py to get \"q_{d}_all.dat\" and \"q_{d}_{label}.dat\" files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv [~/.conda/envs/cenv/]",
   "language": "python",
   "name": "conda_cenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
